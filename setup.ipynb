{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dasheth/miniconda3/envs/unlearning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 49/49 [00:00<00:00, 91.44files/s]\n",
      "Downloading data: 100%|██████████| 16/16 [00:00<00:00, 165.23files/s]\n",
      "Generating train split: 100%|██████████| 988/988 [00:00<00:00, 19268.20 examples/s]\n",
      "Generating dev split: 100%|██████████| 320/320 [00:00<00:00, 21375.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# load the QA pairs\n",
    "semeval_dev_qa = load_dataset(\"cardiffnlp/databench\", name=\"semeval\", split=\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[1;32m      3\u001b[0m llm \u001b[38;5;241m=\u001b[39m Llama\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m \trepo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/stable-code-3b-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m \tfilename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstable-code-3b.Q2_K.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "\trepo_id=\"TheBloke/stable-code-3b-GGUF\",\n",
    "\tfilename=\"stable-code-3b.Q2_K.gguf\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FileNotFoundError' object has no attribute 'stderr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mcall_gguf_model\u001b[0;34m(prompts)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(result\u001b[38;5;241m.\u001b[39mstdout)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlearning/lib/python3.11/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/unlearning/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlearning/lib/python3.11/subprocess.py:1955\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1955\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'llama-cli'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 91\u001b[0m\n\u001b[1;32m     71\u001b[0m runner \u001b[38;5;241m=\u001b[39m Runner(\n\u001b[1;32m     72\u001b[0m     model_call\u001b[38;5;241m=\u001b[39mcall_gguf_model,\n\u001b[1;32m     73\u001b[0m     prompt_generator\u001b[38;5;241m=\u001b[39mexample_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m runner_lite \u001b[38;5;241m=\u001b[39m Runner(\n\u001b[1;32m     82\u001b[0m     model_call\u001b[38;5;241m=\u001b[39mcall_gguf_model,\n\u001b[1;32m     83\u001b[0m     prompt_generator\u001b[38;5;241m=\u001b[39mexample_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     89\u001b[0m )\n\u001b[0;32m---> 91\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredictions.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m responses_lite \u001b[38;5;241m=\u001b[39m runner_lite\u001b[38;5;241m.\u001b[39mrun(save\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions_lite.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataBench accuracy is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator\u001b[38;5;241m.\u001b[39meval(responses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# ~0.15\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/unlearning/lib/python3.11/site-packages/databench_eval/run.py:61\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self, prompts, save)\u001b[0m\n\u001b[1;32m     59\u001b[0m         batch_prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_generator(row) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m batch_rows]\n\u001b[1;32m     60\u001b[0m         batch_datasets \u001b[38;5;241m=\u001b[39m [row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m batch_rows]\n\u001b[0;32m---> 61\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_datasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_responses(save)\n",
      "File \u001b[0;32m~/miniconda3/envs/unlearning/lib/python3.11/site-packages/databench_eval/run.py:29\u001b[0m, in \u001b[0;36mRunner.process_prompt\u001b[0;34m(self, prompts, datasets)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_prompt\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts, datasets):\n\u001b[0;32m---> 29\u001b[0m     raw_responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     responses \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(response\u001b[38;5;241m=\u001b[39mraw_response, dataset\u001b[38;5;241m=\u001b[39mdataset)\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m raw_response, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(raw_responses, datasets)\n\u001b[1;32m     33\u001b[0m     ]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39mextend(prompts)\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mcall_gguf_model\u001b[0;34m(prompts)\u001b[0m\n\u001b[1;32m     19\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(result\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 21\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__CODE_GEN_ERROR__: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstderr\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FileNotFoundError' object has no attribute 'stderr'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import shlex\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from databench_eval import Runner, Evaluator, utils\n",
    "\n",
    "\n",
    "# this makes use of https://huggingface.co/TheBloke/stable-code-3b-GGUF\n",
    "# and https://github.com/ggerganov/llama.cpp\n",
    "def call_gguf_model(prompts):\n",
    "    results = []\n",
    "    for p in prompts:\n",
    "        escaped = p.replace('\"', '\\\\\"')\n",
    "        cmd = f'llama-cli -m ./models/stable-code-3b.Q4_K_M.gguf -p \"{escaped}\" -c 1024 -n 128'\n",
    "        args = shlex.split(cmd)\n",
    "        try:\n",
    "            result = subprocess.run(args, capture_output=True, text=True, check=True)\n",
    "            results.append(result.stdout)\n",
    "        except Exception as e:\n",
    "            results.append(f\"__CODE_GEN_ERROR__: {e.stderr}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def chat_completions(prompts, model=\"gpt-4o-mini\", max_tokens=150):\n",
    "  \"\"\"\n",
    "  This function takes a list of prompts and returns a list of corresponding generations using OpenAI's Chat Completions API.\n",
    "\n",
    "  Args:\n",
    "      prompts (list): A list of user prompts for the chat conversation (strings).\n",
    "      api_key (str, optional): Your OpenAI API key. Defaults to \"YOUR_API_KEY\".\n",
    "      model (str, optional): The OpenAI model to use. Defaults to \"gpt-4o-mini\".\n",
    "      max_tokens (int, optional): The maximum number of tokens for the generated response. Defaults to 150.\n",
    "\n",
    "  Returns:\n",
    "      list: A list of generated responses (strings), corresponding to the input prompts.\n",
    "\n",
    "  Raises:\n",
    "      ValueError: If any prompt exceeds the maximum character limit.\n",
    "  \"\"\"\n",
    "\n",
    "  # Error handling for prompt length\n",
    "  max_char_limit = max_tokens * 4  # Assuming 4 characters per token (average)\n",
    "  for prompt in prompts:\n",
    "    if len(prompt) > max_char_limit:\n",
    "      raise ValueError(f\"Prompt '{prompt}' exceeds the maximum character limit of {max_char_limit}\")\n",
    "\n",
    "  api_key = os.getenv(\"OPENAI_API_KEY\", api_key)\n",
    "  client = OpenAI(api_key)\n",
    "  generations = []\n",
    "\n",
    "  for prompt in prompts:\n",
    "    response = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "      max_tokens=max_tokens\n",
    "    )\n",
    "    generations.append(response.choices[0].message.content.strip())  # Remove leading/trailing whitespace\n",
    "\n",
    "  return generations\n",
    "\n",
    "# # Example usage\n",
    "# prompts = [\"What is the capital of France?\", \"Write a haiku about nature.\"\n",
    "# generations = chat_completions(prompts, api_key)\n",
    "\n",
    "# print(generations)\n",
    "\n",
    "\n",
    "def example_generator(row: dict) -> str:\n",
    "    \"\"\"IMPORTANT:\n",
    "    **Only the question and dataset keys will be available during the actual competition**.\n",
    "    You can, however, try to predict the answer type or columns used\n",
    "    with another modeling task if that helps, then use them here.\n",
    "    \"\"\"\n",
    "    dataset = row[\"dataset\"]\n",
    "    question = row[\"question\"]\n",
    "    df = utils.load_table(dataset)\n",
    "    return f\"\"\"\n",
    "# TODO: complete the following function in one line. It should give the answer to: How many rows are there in this dataframe? \n",
    "def example(df: pd.DataFrame) -> int:\n",
    "    df.columns=[\"A\"]\n",
    "    return df.shape[0]\n",
    "\n",
    "# TODO: complete the following function in one line. It should give the answer to: {question}\n",
    "def answer(df: pd.DataFrame) -> {row[\"type\"]}:\n",
    "    df.columns = {list(df.columns)}\n",
    "    return\"\"\"\n",
    "\n",
    "\n",
    "def example_postprocess(response: str, dataset: str, loader):\n",
    "    try:\n",
    "        df = loader(dataset)\n",
    "        lead = \"\"\"\n",
    "def answer(df):\n",
    "    return \"\"\"\n",
    "        exec(\n",
    "            \"global ans\\n\"\n",
    "            + lead\n",
    "            + response.split(\"return\")[2]\n",
    "            .split(\"\\n\")[0]\n",
    "            .strip()\n",
    "            .replace(\"[end of text]\", \"\")\n",
    "            + f\"\\nans = answer(df)\"\n",
    "        )\n",
    "        # no true result is > 1 line atm, needs 1 line for txt format\n",
    "        return ans.split(\"\\n\")[0] if \"\\n\" in str(ans) else ans\n",
    "    except Exception as e:\n",
    "        return f\"__CODE_ERROR__: {e}\"\n",
    "\n",
    "\n",
    "qa = utils.load_qa(name=\"semeval\", split=\"dev\")\n",
    "evaluator = Evaluator(qa=qa)\n",
    "\n",
    "runner = Runner(\n",
    "    # model_call=call_gguf_model,\n",
    "    model_call=chat_completions,\n",
    "    prompt_generator=example_generator,\n",
    "    postprocess=lambda response, dataset: example_postprocess(\n",
    "        response, dataset, utils.load_table\n",
    "    ),\n",
    "    qa=qa,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "runner_lite = Runner(\n",
    "    # model_call=call_gguf_model,\n",
    "    model_call=chat_completions,\n",
    "    prompt_generator=example_generator,\n",
    "    postprocess=lambda response, dataset: example_postprocess(\n",
    "        response, dataset, utils.load_sample\n",
    "    ),\n",
    "    qa=qa,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "responses = runner.run(save=\"predictions.txt\")\n",
    "responses_lite = runner_lite.run(save=\"predictions_lite.txt\")\n",
    "print(f\"DataBench accuracy is {evaluator.eval(responses)}\")  # ~0.15\n",
    "print(\n",
    "    f\"DataBench_lite accuracy is {evaluator.eval(responses_lite, lite=True)}\"\n",
    ")  # ~0.07\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(\"Archive.zip\", \"w\") as zipf:\n",
    "    zipf.write(\"predictions.txt\")\n",
    "    zipf.write(\"predictions_lite.txt\")\n",
    "\n",
    "print(\"Created Archive.zip containing predictions.txt and predictions_lite.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
